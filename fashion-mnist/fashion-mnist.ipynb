{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Постройте модель для классификации FashionMNIST. Попробуйте получить качество на тестовой выборке не ниже 88%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision as tv\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_dataset = tv.datasets.FashionMNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
    "test_dataset = tv.datasets.FashionMNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1dbf940d640>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test_dataset.data.shape)\n",
    "plt.imshow(test_dataset.data[300].numpy(), cmap='gray_r')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1, time: 7.6,  TRAIN_LOSS=0.587 TRAIN_ACC=0.7866 TEST_LOSS=0.4242 TEST_ACC=0.8477\n",
      "ep: 2, time: 8.4,  TRAIN_LOSS=0.3998 TRAIN_ACC=0.8541 TEST_LOSS=0.3876 TEST_ACC=0.8589\n",
      "ep: 3, time: 7.9,  TRAIN_LOSS=0.364 TRAIN_ACC=0.8668 TEST_LOSS=0.3911 TEST_ACC=0.8613\n",
      "ep: 4, time: 8.0,  TRAIN_LOSS=0.3426 TRAIN_ACC=0.8726 TEST_LOSS=0.3655 TEST_ACC=0.8684\n",
      "ep: 5, time: 8.0,  TRAIN_LOSS=0.3279 TRAIN_ACC=0.8788 TEST_LOSS=0.3692 TEST_ACC=0.8651\n",
      "ep: 6, time: 7.9,  TRAIN_LOSS=0.3175 TRAIN_ACC=0.8821 TEST_LOSS=0.3619 TEST_ACC=0.8692\n",
      "ep: 7, time: 7.8,  TRAIN_LOSS=0.3051 TRAIN_ACC=0.8873 TEST_LOSS=0.3495 TEST_ACC=0.8729\n",
      "ep: 8, time: 7.8,  TRAIN_LOSS=0.2965 TRAIN_ACC=0.8899 TEST_LOSS=0.3435 TEST_ACC=0.8774\n",
      "ep: 9, time: 8.0,  TRAIN_LOSS=0.2876 TRAIN_ACC=0.8923 TEST_LOSS=0.3506 TEST_ACC=0.8744\n",
      "ep: 10, time: 8.4,  TRAIN_LOSS=0.2825 TRAIN_ACC=0.8949 TEST_LOSS=0.3317 TEST_ACC=0.8817\n",
      "ep: 11, time: 8.4,  TRAIN_LOSS=0.2757 TRAIN_ACC=0.8968 TEST_LOSS=0.332 TEST_ACC=0.8772\n",
      "ep: 12, time: 8.0,  TRAIN_LOSS=0.2723 TRAIN_ACC=0.898 TEST_LOSS=0.3258 TEST_ACC=0.8829\n",
      "ep: 13, time: 7.8,  TRAIN_LOSS=0.2632 TRAIN_ACC=0.9011 TEST_LOSS=0.3197 TEST_ACC=0.8859\n",
      "ep: 14, time: 8.3,  TRAIN_LOSS=0.2598 TRAIN_ACC=0.9023 TEST_LOSS=0.3265 TEST_ACC=0.8826\n",
      "ep: 15, time: 8.0,  TRAIN_LOSS=0.2581 TRAIN_ACC=0.9022 TEST_LOSS=0.3166 TEST_ACC=0.8863\n",
      "ep: 16, time: 7.9,  TRAIN_LOSS=0.2524 TRAIN_ACC=0.9043 TEST_LOSS=0.3262 TEST_ACC=0.8823\n",
      "ep: 17, time: 7.9,  TRAIN_LOSS=0.2467 TRAIN_ACC=0.9056 TEST_LOSS=0.3205 TEST_ACC=0.8844\n",
      "ep: 18, time: 8.2,  TRAIN_LOSS=0.2403 TRAIN_ACC=0.9088 TEST_LOSS=0.3248 TEST_ACC=0.8847\n",
      "ep: 19, time: 8.1,  TRAIN_LOSS=0.2421 TRAIN_ACC=0.9077 TEST_LOSS=0.3188 TEST_ACC=0.8896\n",
      "ep: 20, time: 8.1,  TRAIN_LOSS=0.2366 TRAIN_ACC=0.9099 TEST_LOSS=0.3225 TEST_ACC=0.8861\n",
      "ep: 21, time: 8.1,  TRAIN_LOSS=0.2348 TRAIN_ACC=0.9109 TEST_LOSS=0.3221 TEST_ACC=0.8881\n",
      "ep: 22, time: 8.2,  TRAIN_LOSS=0.2349 TRAIN_ACC=0.9112 TEST_LOSS=0.3175 TEST_ACC=0.8899\n",
      "ep: 23, time: 8.2,  TRAIN_LOSS=0.2283 TRAIN_ACC=0.9123 TEST_LOSS=0.3225 TEST_ACC=0.8882\n",
      "ep: 24, time: 7.9,  TRAIN_LOSS=0.2225 TRAIN_ACC=0.9151 TEST_LOSS=0.3181 TEST_ACC=0.8892\n",
      "ep: 25, time: 8.3,  TRAIN_LOSS=0.2218 TRAIN_ACC=0.9153 TEST_LOSS=0.3317 TEST_ACC=0.8892\n",
      "ep: 26, time: 8.0,  TRAIN_LOSS=0.221 TRAIN_ACC=0.9161 TEST_LOSS=0.323 TEST_ACC=0.8883\n",
      "ep: 27, time: 8.0,  TRAIN_LOSS=0.2142 TRAIN_ACC=0.9176 TEST_LOSS=0.3267 TEST_ACC=0.892\n",
      "ep: 28, time: 8.1,  TRAIN_LOSS=0.2115 TRAIN_ACC=0.9195 TEST_LOSS=0.3286 TEST_ACC=0.8847\n",
      "ep: 29, time: 8.4,  TRAIN_LOSS=0.2125 TRAIN_ACC=0.9192 TEST_LOSS=0.3185 TEST_ACC=0.8917\n",
      "ep: 30, time: 7.8,  TRAIN_LOSS=0.2079 TRAIN_ACC=0.9204 TEST_LOSS=0.3172 TEST_ACC=0.8933\n",
      "ep: 31, time: 8.1,  TRAIN_LOSS=0.2058 TRAIN_ACC=0.9212 TEST_LOSS=0.3325 TEST_ACC=0.888\n",
      "ep: 32, time: 8.4,  TRAIN_LOSS=0.2046 TRAIN_ACC=0.921 TEST_LOSS=0.3227 TEST_ACC=0.8883\n",
      "ep: 33, time: 8.3,  TRAIN_LOSS=0.2014 TRAIN_ACC=0.9236 TEST_LOSS=0.3232 TEST_ACC=0.8914\n",
      "ep: 34, time: 8.1,  TRAIN_LOSS=0.1988 TRAIN_ACC=0.9236 TEST_LOSS=0.3275 TEST_ACC=0.894\n",
      "ep: 35, time: 8.1,  TRAIN_LOSS=0.1985 TRAIN_ACC=0.9233 TEST_LOSS=0.336 TEST_ACC=0.8863\n",
      "ep: 36, time: 8.0,  TRAIN_LOSS=0.1948 TRAIN_ACC=0.9248 TEST_LOSS=0.3331 TEST_ACC=0.8872\n",
      "ep: 37, time: 8.0,  TRAIN_LOSS=0.1914 TRAIN_ACC=0.9272 TEST_LOSS=0.333 TEST_ACC=0.8876\n",
      "ep: 38, time: 7.8,  TRAIN_LOSS=0.1919 TRAIN_ACC=0.9267 TEST_LOSS=0.3341 TEST_ACC=0.89\n",
      "ep: 39, time: 8.1,  TRAIN_LOSS=0.1898 TRAIN_ACC=0.9271 TEST_LOSS=0.3326 TEST_ACC=0.8913\n",
      "ep: 40, time: 7.9,  TRAIN_LOSS=0.1885 TRAIN_ACC=0.9283 TEST_LOSS=0.3175 TEST_ACC=0.8942\n",
      "Total time: 5 min 22 sec\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "LR = 0.002\n",
    "NUM_EPOCHS = 40\n",
    "N1 = 256\n",
    "N2 = 256\n",
    "\n",
    "train = torch.utils.data.DataLoader(train_dataset, BATCH_SIZE)\n",
    "test = torch.utils.data.DataLoader(test_dataset, BATCH_SIZE)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, N1),\n",
    "    # torch.nn.BatchNorm1d(N1),\n",
    "    torch.nn.Dropout(p=.25),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N1, N2),\n",
    "    # torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N2, 10),\n",
    ")\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# trainer = torch.optim.SGD(model.parameters(), LR)\n",
    "trainer = torch.optim.Adam(model.parameters(), LR)\n",
    "# trainer = torch.optim.RMSprop(model.parameters(), LR)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_iter, train_passed = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for F, L in train:\n",
    "        trainer.zero_grad()\n",
    "        L_calc = model(F)\n",
    "        l = loss(L_calc, L)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "\n",
    "        train_loss += l.item()\n",
    "        train_acc += (L_calc.argmax(dim=1) == L).sum().item()\n",
    "        train_iter += 1\n",
    "        train_passed += len(F)\n",
    "\n",
    "\n",
    "    test_iter, test_passed = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "\n",
    "    model.eval()\n",
    "    for F, L in test:\n",
    "        L_calc = model(F)\n",
    "        l = loss(L_calc, L)\n",
    "\n",
    "        test_loss += l.item()\n",
    "        test_acc += (L_calc.argmax(dim=1) == L).sum().item()\n",
    "        test_iter += 1\n",
    "        test_passed += len(F)\n",
    "\n",
    "    print(f'ep: {epoch}, time: {round(time.time() - epoch_time, 1)}, '\n",
    "          f' TRAIN_LOSS={round(train_loss/train_iter, 4)}'\n",
    "          f' TRAIN_ACC={round(train_acc/train_passed, 4)}'\n",
    "          f' TEST_LOSS={round(test_loss/test_iter, 4)}'\n",
    "          f' TEST_ACC={round(test_acc/test_passed, 4)}')\n",
    "\n",
    "total_time = round(time.time() - start_time)\n",
    "print(f'Total time: {total_time // 60} min {total_time % 60} sec')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Начал с параметров из примеров с цифрами.\n",
    "Сначала подобрал LR, при 0.002 точность росла довольно быстро и относительно стабильно. При больших значениях на подходе к 88% начинало колбасить, при меньших рост качества сильно замедлялся.\n",
    "Добавил еще один слой -- результат стал стабильнее. Начинал от 16 нейронов на втором слое, но увеличение количества сказалось положительно на качестве, так что остановился так же на 256.\n",
    "После 10 эпох качество выходит на 88%, редко переваливало 89% в пределах 20 эпох.\n",
    "\n",
    "При обучении на тренировочной выборке размер потерь уменьшается, а качество растет, что в принципе логично. А вот на тестовой выборке качество растет, но потери увеличиваются. Пытаюсь понять что это означает..\n",
    "\n",
    "Вспомнил про регуляризацию, добавил Dropout с вроятностью по умолчанию - 0.5.\n",
    "Сначала после второго слоя - результат стал еще стабильнее и потери чуть снизились и стабилизировались.\n",
    "Потом только после первого слоя - скорость обучения упала в два раза, т.е. качество на 88% выходило ближе к 20 эпохе, но при этом все шло очень стабильно и потери продолжали уменьшаться с ростом качества.\n",
    "Добавление дропаута после обоих слоев ухудшило результат. Оставил только после первого слоя из-за стабильности и уменьшения потерь, хоть и медленнее -- увеличил количество эпох до 40.\n",
    "Надо еще поиграться с параметром вероятности дропдауна..\n",
    "Значение вероятности 0.25 увеличило скорость обучения и дало стабильное качество в 89% после 30 эпохи в некоторых случаях.\n",
    "\n",
    "Попробовал использовать Batch Normalization. Возможно ее нужно правильно настроить, но с параметрами по умолчанию она ухудшила результат."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}